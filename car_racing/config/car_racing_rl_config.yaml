car_racing:
  rl_trainers:
    ros__parameters:
      metadata:
        mode: eval            # Run mode: 'train' to learn, 'eval' to evaluate
        task: car_racing       # Task/Spec name (must be registered as an rl_task)
        artifacts_dir: /app/src/car_racing/data  # Root folder for logs and saved models
      algorithm:
        name: ppo              # RL algorithm: ppo | a2c | dqn | sac | td3 (must fit action space)
        device: auto           # 'cpu', 'cuda', or 'auto' (use GPU if available)
        policy: CnnPolicy      # Network type: CnnPolicy for image inputs, MlpPolicy for vector inputs
      train:
        total_timesteps: 1200000  # Total env steps for training (across all parallel envs)
        progress: true            # Show progress bar/logging during training
        n_envs: 6                 # Number of parallel environments (VecEnv) for faster sampling
      eval:
        episodes: 5               # Number of episodes to run during evaluation
        deterministic: true       # Use deterministic policy at eval (no exploration noise)
        model_path: /app/src/car_racing/data/model/PPO/car_racing_20250915_09_19  # Run folder or direct 'model(.zip)' path
        n_envs: 1                 # Usually 1 for eval (simpler render/ROS); >1 for headless comparisons only
      ppo:
        n_steps: 2048          # Rollout length PER env before an update (6 envs → 2048*6 samples/update)
        batch_size: 2048       # Minibatch size per gradient update (must divide n_envs*n_steps)
        learning_rate: 3.0e-4  # Optimizer step size (too high = unstable, too low = slow)
        gamma: 0.99            # Discount factor for future rewards (closer to 1 = more long-term)
        gae_lambda: 0.95       # GAE parameter (bias–variance trade-off in advantage estimates)
        clip_range: 0.2        # PPO policy clipping range (too small = slow, too large = unstable)
        ent_coef: 0.0          # Entropy bonus coefficient (higher = more exploration)
        vf_coef: 0.5           # Weight of value-function loss in total loss
        n_epochs: 10           # Number of passes over collected data per update
        use_sde: false         # State-Dependent Exploration (mainly for continuous action spaces)
        policy_kwargs: '{"normalize_images": true}'  # Extra policy args (here: scale images 0..255 → 0..1)
